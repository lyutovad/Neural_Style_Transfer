{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jo5PziEC4hWs"
   },
   "source": [
    "# Neural Style Transfer with tf.keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aDyGj8DmXCJI"
   },
   "source": [
    "## Обзор\n",
    "\n",
    "Статья Леона А. Гэтиса https://arxiv.org/abs/1508.06576\n",
    "\n",
    "Neural style transfer (Перенос нейронного стиля) - это метод оптимизации, используемый для получения трех изображений, изображения **содержимого**, изображения **референса стиля** (например, работы известного художника) и **входного изображения**, которое хотите стилизовать - и смешать их вместе так, чтобы входное изображение было преобразовано, чтобы выглядеть как изображение содержимого, но «нарисовано» в стиле изображения стиля.\n",
    "\n",
    "\n",
    "Например, возьмем изображение этой черепахи и «Великой волны у Канагавы» Кацусики Хокусая:\n",
    "\n",
    "<img src=\"https://github.com/tensorflow/models/blob/master/research/nst_blogpost/Green_Sea_Turtle_grazing_seagrass.jpg?raw=1\" alt=\"Drawing\" style=\"width: 200px;\"/>\n",
    "<img src=\"https://github.com/tensorflow/models/blob/master/research/nst_blogpost/The_Great_Wave_off_Kanagawa.jpg?raw=1\" alt=\"Drawing\" style=\"width: 200px;\"/>\n",
    "\n",
    "[Image of Green Sea Turtle](https://commons.wikimedia.org/wiki/File:Green_Sea_Turtle_grazing_seagrass.jpg)\n",
    "-By P.Lindgren [CC BY-SA 3.0  (https://creativecommons.org/licenses/by-sa/3.0)], from Wikimedia Commons\n",
    "\n",
    "Как бы это выглядело, если бы Хокусай решил нарисовать эту черепаху исключительно в этом стиле? Что-то вроде этого?\n",
    "\n",
    "<img src=\"https://github.com/tensorflow/models/blob/master/research/nst_blogpost/wave_turtle.png?raw=1\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "Это магия или просто глубокое обучение? К счастью, это не связано с каким-либо колдовством: перенос стиля - это забавный и интересный метод, демонстрирующий возможности и внутренние представления нейронных сетей.\n",
    "\n",
    "Принцип переноса нейронного стиля состоит в том, чтобы определить две функции расстояния, одна из которых описывает, насколько различается содержимое двух изображений, $L_{content}$, а другая описывает разницу между двумя изображениями с точки зрения их стиля, $L_{style}$. Затем, имея три изображения, желаемое изображение стиля, желаемое изображение содержимого и входное изображение (инициализированное изображением содержимого), мы пытаемся преобразовать входное изображение, чтобы минимизировать расстояние содержимого с изображением содержимого и расстояние его стиля с помощью стиль изображения.\n",
    "Таким образом, мы возьмем базовое входное изображение, изображение контента, которое мы хотим сопоставить, и изображение стиля, которое мы хотим сопоставить. Мы преобразуем базовое входное изображение, минимизируя расстояния (потери) между содержимым и стилями с помощью обратного распространения, создавая изображение, которое соответствует содержимому изображения содержимого и стилю изображения стиля.\n",
    "\n",
    "### Конкретные концепции, которые будут рассмотрены:\n",
    "В процессе мы будем накапливать практический опыт и развивать интуицию вокруг следующих концепций.\n",
    "\n",
    "* **Eager Execution** - используйте среду обязательного программирования TensorFlow, которая момментально оценивает операции\n",
    "   *[Подробнее о Eager Execution] (https://www.tensorflow.org/programmers_guide/eager)\n",
    "   *[В действии] (https://www.tensorflow.org/get_started/eager)\n",
    "* **Используя [Функциональный API] (https://keras.io/getting-started/functional-api-guide/) для определения модели** - мы создадим подмножество нашей модели, которое даст нам доступ к необходимой промежуточной активации с использованием функционального API\n",
    "* **Использование карт признаков предварительно обученной модели** - Узнайте, как использовать предварительно обученные модели и их карты признаков.\n",
    "* **Создание пользовательских циклов обучения** - мы рассмотрим, как настроить оптимизатор, чтобы минимизировать заданные потери в соответствии с параметрами входа.\n",
    "\n",
    "### Шаги для переноса стиля:\n",
    "\n",
    "1. Визуализация данных.\n",
    "2. Базовая предварительная обработка/ подготовка наших данных.\n",
    "3. Настройка функции потерь.\n",
    "4. Создание модели.\n",
    "5. Оптимизация функции потерь\n",
    "\n",
    "Доп. информация:\n",
    "* Прочтите [статью Гэтиса] (https://arxiv.org/abs/1508.06576) - мы объясним по ходу дела, но документ предоставит более полное понимание задачи.\n",
    "* [Понять, как уменьшить потери с помощью градиентного спуска] (https://developers.google.com/machine-learning/crash-course/reeding-loss/gradient-descent)\n",
    "\n",
    "** Расчетное время **: 30 мин."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U8ajP_u73s6m"
   },
   "source": [
    "## Установка\n",
    "\n",
    "### Загрузка изображений"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eqxUicSPUOP6"
   },
   "source": [
    "### Импорт и настройка модулей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "sc1OLbOWhPCO"
   },
   "outputs": [],
   "source": [
    "# Для визуализации и графиков\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['figure.figsize'] = (10,10) # определяем размер отображаемой картинки\n",
    "mpl.rcParams['axes.grid'] = False # убираем сетку (это изменение навтройки для matplotlib,  делается 1 раз)\n",
    "\n",
    "# поддержка больших многомерных массивов и матриц\n",
    "import numpy as np\n",
    "# для обработки графики в Python\n",
    "from PIL import Image\n",
    "# для работы со временем\n",
    "import time\n",
    "# инструменты для адаптации или расширения функций и других вызываемых объектов\n",
    "import functools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "RYEjlrYk3s6w"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%tensorflow_version` not found.\n"
     ]
    }
   ],
   "source": [
    "%tensorflow_version 1.x \n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.python.keras.preprocessing import image as kp_image\n",
    "from tensorflow.python.keras import models \n",
    "from tensorflow.python.keras import losses\n",
    "from tensorflow.python.keras import layers\n",
    "from tensorflow.python.keras import backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L7sjDODq67HQ"
   },
   "source": [
    "Мы начнем с включения [eager execution](https://www.tensorflow.org/guide/eager). Eager execution позволяет нам проработать эту технику самым ясным и понятным образом. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "sfjsSAtNrqQx"
   },
   "outputs": [],
   "source": [
    "# tf.enable_eager_execution()\n",
    "# print(\"Eager execution: {}\".format(tf.executing_eagerly()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "IOiGrIV1iERH"
   },
   "outputs": [],
   "source": [
    "# Пути для входной картинки и картинки со стилем \n",
    "content_path = './images/Green_Sea_Turtle_grazing_seagrass.jpg' # путь к входному изображению\n",
    "style_path = './images/The_Great_Wave_off_Kanagawa.jpg' # путь к стилевому изображению"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xE4Yt8nArTeR"
   },
   "source": [
    "## Визуализация входной картинки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "3TLljcwv5qZs"
   },
   "outputs": [],
   "source": [
    "def load_img(path_to_img):\n",
    "    '''\n",
    "    Загрузка и предобработка изображения\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    path_to_img: str \n",
    "        Путь к изображению\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    img: numpy.array, shape = [1, height, width, channels]\n",
    "        массив, соответствующий изображению\n",
    "    '''\n",
    "    max_dim = 512 # максимальная размерность изображения\n",
    "    img = Image.open(path_to_img) # открываем изобр. библиотекой PIL\n",
    "    long = max(img.size) # возвращает ммаксимальный элемент (длину или ширину)\n",
    "    scale = max_dim/long # я так понимаю, что это типа нормализации, тут делим заданную макс размерномть на полученный максимальный эелемент\n",
    "    img = img.resize((round(img.size[0]*scale), round(img.size[1]*scale)), Image.ANTIALIAS) # изменение изображения, где применяется наша нормализация. 3 параметр количество каналов, остается неизменным\n",
    "    \n",
    "    img = kp_image.img_to_array(img) # Преобразует экземпляр изображения PIL в массив Numpy.\n",
    "\n",
    "    # Нужно расширить массив изображений, чтобы он имел заданную размерность для объявления батча. Добаляем новую размерность по оси 0 (строка)\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    return img # возвращает массив, соответствующий изображению"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vupl0CI18aAG"
   },
   "outputs": [],
   "source": [
    "def imshow(img, title=None):\n",
    "    '''\n",
    "    Вывод изображения на экран\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    img: numpy.array, shape = [1, height, width, channels]\n",
    "        Массив, соответсвующий изображению\n",
    "    title: str\n",
    "        Заголовок изображения. По умолчанию: None\n",
    "    '''\n",
    "    \n",
    "    # Удаляем размерности (батча)\n",
    "    out = np.squeeze(img, axis=0) # удаляет оси с одним элементом (длинной 1), но не сами элементы массива. т.е. тут удаляем размерность, заданную для батча\n",
    "    # Нормализация для вывода \n",
    "    out = out.astype('uint8') # тип uint8 - целые числа в диапазоне от 0 по 255 (числа размером 1 байт)\n",
    "    plt.imshow(out) # строим график\n",
    "    if title is not None: # Если есть заголовок, выводим его\n",
    "        plt.title(title)\n",
    "    plt.imshow(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2yAlRzJZrWM3"
   },
   "source": [
    "Это входной контент и изображения стиля. Мы надеемся «создать» изображение с нашим содержимым, но свыбранным стилем."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_UWQmeEaiKkP"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10)) # размер отображаемого изображения\n",
    "\n",
    "content = load_img(content_path).astype('uint8') # загружаем подготовленной функцией контент \n",
    "style = load_img(style_path).astype('uint8') #  и изображение\n",
    "\n",
    "plt.subplot(1, 2, 1) # изображение в первом ряду, в 1 (из 2) столбце\n",
    "imshow(content, 'Content Image') # контент\n",
    "\n",
    "plt.subplot(1, 2, 2) # изображение в первом ряду, в 2 (из 2) столбце\n",
    "imshow(style, 'Style Image') # стиль\n",
    "plt.show() # вывод изображения"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7qMVNvEsK-_D"
   },
   "source": [
    "## Подготовка данных\n",
    "Создаем функции для загрузки и предварительной обраотки изображения. Мы выполняем тот же процесс предварительной обработки, который ожидается в соответствии с процессом обучения VGG. Сети VGG обучаются на изображении с каждым каналом, нормализованным на \"среднее значение = [103.939, 116.779, 123.68]\", и с каналами BGR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "hGwmTwJNmv2a"
   },
   "outputs": [],
   "source": [
    "def load_and_process_img(path_to_img):\n",
    "    '''\n",
    "    Загрузка и обработка изображения\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    path_to_img: str \n",
    "        Путь к изображению\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    img: numpy.array, shape = [1, height, width, channels]\n",
    "        Нормализованный массив, соответсвующий изображению, с каналами BGR,\n",
    "        с каждым каналом, нормализованным на среднее значение = [103.939, 116.779, 123.68]\n",
    "    '''\n",
    "    img = load_img(path_to_img) # загрузка изображения подготовленной функцией load_img\n",
    "    img = tf.keras.applications.vgg19.preprocess_input(img) # обрабатывает массив в соответсвии с требованиями сети VGG\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xCgooqs6tAka"
   },
   "source": [
    "Чтобы просмотреть результаты оптимизации, необходимо выполнить обратный шаг предварительной обработки. Кроме того, поскольку наше оптимизированное изображение может принимать значения где угодно между $ - \\ infty $ и $ \\ infty $, мы должны его обрезать, чтобы сохранить значения в диапазоне 0–255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mjzlKRQRs_y2"
   },
   "outputs": [],
   "source": [
    "def deprocess_img(processed_img):\n",
    "    '''\n",
    "    Инверсия препроцессинга\n",
    "     Parameters\n",
    "    ----------\n",
    "    processed_img: numpy.array, shape = [1, height, width, channels]\n",
    "        Нормализованный массив, соответсвующий изображению, с каналами BGR,\n",
    "        с каждым каналом, нормализованным на среднее значение = [103.939, 116.779, 123.68]\n",
    "               \n",
    "    Returns\n",
    "    -------\n",
    "    x: numpy.array, shape = [1, height, width, channels]\n",
    "        Массив, соответсвующий изображению\n",
    "    '''\n",
    "    \n",
    "    x = processed_img.copy() # создание копии обработанного изображения\n",
    "    if len(x.shape) == 4: # проверяемразмерность. если длина размерности == 4\n",
    "        x = np.squeeze(x, 0) # сжимаем одну размерность (которая предназначена для батча) \n",
    "    assert len(x.shape) == 3, # проверяем размерность массива. На данном этапе должно остаться 3 размерности height, width, channels\n",
    "    \n",
    "    if len(x.shape) != 3: # Если длина размерностей не  равна 3\n",
    "        raise ValueError(\"Invalid input to deprocessing image\") # вызываем ошибку: Неправильный вход на депроцессинг изображения\n",
    "\n",
    "    # инверсия шага предварительной обработки Для каждого канала прибавляем среднее значение, на когорое нормировали и переставляем каалы опять в RGB\n",
    "    x[:, :, 0] += 103.939\n",
    "    x[:, :, 1] += 116.779\n",
    "    x[:, :, 2] += 123.68\n",
    "    x = x[:, :, ::-1]\n",
    "\n",
    "    x = np.clip(x, 0, 255).astype('uint8') # ограниченичивает элементы массива 0 и 255, переводит в тип uint8\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GEwZ7FlwrjoZ"
   },
   "source": [
    "### Определение представлений содержимого и стиля\n",
    "Чтобы получить представление как содержимого, так и стиля нашего изображения, мы рассмотрим некоторые промежуточные слои в нашей модели. По мере того, как мы углубляемся в модель, эти промежуточные слои представляют особенности более и более высокого высокого порядка. В этом случае мы используем сетевую архитектуру VGG19, предварительно обученную сеть классификации изображений. Эти промежуточные слои необходимы для определения представления контента и стиля наших изображений. Для входного изображения мы попытаемся сопоставить соответствующие представления стиля и содержимого на этих промежуточных уровнях.\n",
    "\n",
    "<img src=\"image001.jpg\">\n",
    "\n",
    "#### Почему промежуточные слои?\n",
    "\n",
    "Вам может быть интересно, почему эти промежуточные результаты в нашей предварительно обученной сети классификации изображений позволяют нам определять стиль и представления контента. На высоком уровне это явление можно объяснить тем фактом, что для того, чтобы сеть могла выполнять классификацию изображений (что наша сеть была обучена делать), она должна понимать изображение. Это включает в себя использование необработанного изображения в качестве входных пикселей и построение внутреннего представления посредством преобразований, которые превращают необработанные пиксели изображения в комплексное понимание функций, присутствующих в изображении. Отчасти поэтому сверточные нейронные сети способны хорошо обобщать: они способны улавливать инварианты и определять особенности внутри классов (например, кошки против собак), которые не зависят от фонового шума и других неприятностей. Таким образом, где-то между тем, где загружается необработанное изображение и выводится классификационная метка, модель служит комплексным экстрактором признаков, следовательно, получая доступ к промежуточным слоям, мы можем описывать содержание и стиль входных изображений.\n",
    "\n",
    "В частности, мы удалим эти промежуточные уровни из нашей сети:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N4-8eUp_Kc-j"
   },
   "outputs": [],
   "source": [
    "# Слой контента, с которого мы будем вытаскивать карты признаков\n",
    "content_layers = ['block5_conv2'] \n",
    "\n",
    "# Слои стиля, которые нам будут нужны\n",
    "style_layers = ['block1_conv1',\n",
    "                'block2_conv1',\n",
    "                'block3_conv1', \n",
    "                'block4_conv1', \n",
    "                'block5_conv1'\n",
    "               ]\n",
    "\n",
    "num_content_layers = len(content_layers) # количество слоев контента\n",
    "num_style_layers = len(style_layers) # количество слоев стиля"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jt3i3RRrJiOX"
   },
   "source": [
    "## Построение модели\n",
    "В этом случае мы загружаем [VGG19] (https://keras.io/applications/#vgg19) и передаем в модель наш входной тензор. Это позволит нам извлекать карты признаков (и впоследствии представления контента и стиля) контента, стиля и сгенерированных изображений.\n",
    "\n",
    "Мы используем VGG19, как предлагается в статье. Кроме того, поскольку VGG19 является относительно простой моделью (по сравнению с ResNet, Inception и т. Д.), Карты признаков на самом деле лучше подходят для передачи стилей."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v9AnzEUU6hhx"
   },
   "source": [
    "Чтобы получить доступ к промежуточным уровням, соответствующим нашим картам функций стиля и содержимого, мы получаем соответствующие выходные данные и используем Keras [**Functional API**] (https://keras.io/getting-started/functional-api- guide /), мы определяем нашу модель с желаемыми активациями вывода.\n",
    "\n",
    "С функциональным API определение модели просто включает определение ввода и вывода:\n",
    "\n",
    "`model = Model(inputs, outputs)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nfec6MuMAbPx"
   },
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    ''' \n",
    "    Создает модель с доступом к промежуточным слоям.\n",
    "\n",
    "    Эта функция загрузит модель VGG19 и получит доступ к промежуточным уровням.\n",
    "    Эти слои затем будут использоваться для создания новой модели, которая будет принимать входное изображение.\n",
    "    и вернет выходные данные этих промежуточных уровней из модели VGG.\n",
    "\n",
    "    Returns:\n",
    "    -----------\n",
    "        Модель keras, которая принимает входные данные изображения и выводит \n",
    "        промежуточные слои стиля и содержимого.\n",
    "    '''\n",
    "    # Загружает модель VGG, натренированную на датасете imagenet\n",
    "    vgg = tf.keras.applications.vgg19.VGG19(include_top=False, weights='imagenet') # include_top=False убирает завершающий полносвязный слой сети, который выдает метки классификации\n",
    "                                                                                   # weights='imagenet' указывает на то, что веса берем, обученные на imagenet\n",
    "    vgg.trainable = False # запрет на обучение сети. Мы ее не обучаем\n",
    "    # Get output layers corresponding to style and content layers \n",
    "    style_outputs = [vgg.get_layer(name).output for name in style_layers] # какие слои мы берем для стиля\n",
    "    content_outputs = [vgg.get_layer(name).output for name in content_layers] # какие слои берем для контента\n",
    "    model_outputs = style_outputs + content_outputs # выход модели - слои стиля + слои контента\n",
    "    # Строим модель \n",
    "    return models.Model(vgg.input, model_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vJdYvJTZ4bdS"
   },
   "source": [
    "## Определение и создание функции потерь (расстояние между содержимым и стилем)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F2Hcepii7_qh"
   },
   "source": [
    "### Потери по контенту"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1FvH-gwXi4nq"
   },
   "source": [
    "Наше определение потери по контенту на самом деле довольно простое. Мы передадим в сеть и желаемое изображение контента, и наше базовое входное изображение. Это вернет промежуточные выходные данные слоя (из слоев, определенных выше) из нашей модели. Затем мы просто берем евклидово расстояние между двумя промежуточными представлениями этих изображений.\n",
    "\n",
    "Более формально потеря по контенту - это функция, которая описывает расстояние от контента до выходного изображения $ x $ и изображения содержимого $ p $. Пусть $ C_ {nn} $ будет предварительно обученной глубокой сверточной нейронной сетью. Опять же, в этом случае мы используем [VGG19] (https://keras.io/applications/#vgg19). Пусть $ X $ - любое изображение, тогда $ C_ {nn} (X) $ - сеть, получающая на вход X. Пусть $ F ^ l_ {ij} (x) \\ in C_ {nn} (x) $ и $ P ^ l_ {ij} (p) \\ in C_ {nn} (p) $ описывают соответствующее промежуточное представление характеристик сети с входами $ x $ и $ p $ на уровне $ l $. Тогда мы формально описываем информационное расстояние (потерю) как: $$L^l_{content}(p, x) = \\sum_{i, j} (F^l_{ij}(x) - P^l_{ij}(p))^2$$\n",
    "\n",
    "Мы выполняем обратное распространение ошибки обычным способом, чтобы минимизировать потерю содержимого. Таким образом, мы изменяем исходное изображение до тех пор, пока оно не сгенерирует аналогичный ответ на определенном уровне (определенном в content_layer), что и исходное изображение контента.\n",
    "\n",
    "Реализовать это можно довольно просто. Опять же, он будет принимать в качестве входных данных карты признаков на слое L в сети со входом x, нашим входным изображением, и p, нашим изображением контента, и возвращать расстояние содержимого."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6KsbqPA8J9DY"
   },
   "source": [
    "### Вычисление потери по контенту\n",
    "Фактически мы добавим потери по контенту на каждом желаемом уровне. Таким образом, на каждой итерации, когда мы передаем наше входное изображение через модель (которая в стремлении является просто `model (input_image)`!), Все потери контента через модель будут правильно вычисляться."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d2mf7JwRMkCd"
   },
   "outputs": [],
   "source": [
    "def get_content_loss(base_content, target):\n",
    "    '''\n",
    "    Вычисляет потери по контенту: среднее значение элементов по различным измерениям тензора от \n",
    "    корня разницы между базовым контеном и таргетом\n",
    "\n",
    "    Inputs:\n",
    "    ----------\n",
    "    base_content: tf.Tensor, shape = [1, height, width, channels]\n",
    "    \n",
    "    target: tf.Tensor, shape = [1, height, width, channels]\n",
    "\n",
    "    Returns: \n",
    "    ----------\n",
    "        Ошибка: float\n",
    "    '''\n",
    "    return tf.reduce_mean(tf.square(base_content - target)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lGUfttK9F8d5"
   },
   "source": [
    "## Потери по стилю"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I6XtkGK_YGD1"
   },
   "source": [
    "Вычисление потери по стилю немного сложнее, но вычисляется по тому же принципу, но на этот раз подаем в сеть базовое входное изображение и изображениее стиля. Однако вместо сравнения необработанных промежуточных выходных данных базового входного изображения и изображения стиля мы вместо этого сравниваем матрицы Грама двух выходных данных.\n",
    "\n",
    "Математически мы описываем потерю стиля базового входного изображения $x$ и изображения стиля $a$ как расстояние между представлением стиля (матрицами Грама) этих изображений. Мы описываем стилевое представление изображения как корреляцию между различными параметрами фильтра, заданными матрицей Грама $G^l$, где $G^l_{ij}$ - это внутренний продукт между векторизованной картой признаков $i$ и $j$ в слое $l$. Мы можем видеть, что $G^l_{ij}$, сгенерированный на карте признаков для данного изображения, представляет собой корреляцию между картами признаков $i$ и $j$.\n",
    "\n",
    "Чтобы создать стиль для нашего базового входного изображения, мы выполняем градиентный спуск по контенту, чтобы преобразовать его в изображение, которое соответствует представлению стиля исходного изображения. Мы делаем это, минимизируя среднеквадратичное расстояние между картой корреляции функций изображения стиля и входным изображением. Вклад каждого слоя в общую потерю стиля описывается следующим образом:\n",
    "$$E_l = \\frac{1}{4N_l^2M_l^2} \\sum_{i,j}(G^l_{ij} - A^l_{ij})^2$$\n",
    "\n",
    "где $G^l_{ij}$ и $A^l_{ij}$ - соответствующие представления стилей в слое $l$ для $x$ и $a$. $N_l$ описывает количество карт признаков, каждая размером $M_l = высота * ширина$. Таким образом, общая потеря стиля на каждом слое составляет\n",
    "$$L_{стиль}(a, x) = \\sum_{l\\in L} w_l E_l$$\n",
    "где мы взвешиваем вклад потерь каждого слоя на некоторый коэффициент $ w_l $. В нашем случае мы взвешиваем каждый слой одинаково ($w_l =\\frac{1}{|L|}$)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F21Hm61yLKk5"
   },
   "source": [
    "### Вычисление потери по стилю\n",
    "Мы также реализуем потерю как метрику расстояния "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N7MOqwKLLke8"
   },
   "outputs": [],
   "source": [
    "def gram_matrix(input_tensor):\n",
    "    '''\n",
    "    Строим матрицу Грамма\n",
    "    Inputs:\n",
    "    ----------\n",
    "    input_tensor: tf.Tensor, shape = [1, height, width, channels]\n",
    "        На вход поступает карта признаков\n",
    "\n",
    "    Returns: \n",
    "    ----------\n",
    "        Матрица Грамма: tf.Tensor\n",
    "    '''\n",
    "    \n",
    "    # Сначала делаем каналы изображений \n",
    "    channels = int(input_tensor.shape[-1]) # берем последний элемент размерности (это количество каналов)\n",
    "    a = tf.reshape(input_tensor, [-1, channels]) # меняем форму массива. 3 столбца, а количество строк подбирается исходя из формы начального массива\n",
    "    n = tf.shape(a)[0]  # первый элемент размерности массива а, полученного на предыдущем шаге (т.е. это количество строк)\n",
    "    gram = tf.matmul(a, a, transpose_a=True) # матрица Грамма = матричное умножение а на транспонированную а\n",
    "    return gram / tf.cast(n, tf.float32) # матрица грамма деленная на количество строк, преобразованное в формат флоат (это усредняет величины)\n",
    "\n",
    "def get_style_loss(base_style, gram_target):\n",
    "    '''\n",
    "    Вычисляет среднее значение квадратов разницы между картами стилей формируемого изображения и стилевого\n",
    "    (Эта функция вызывается для каждого слоя)\n",
    "        \n",
    "    Inputs:\n",
    "    ----------\n",
    "    base_style: tf.Tensor, shape = [height, width, channels] высота, ширина, количество фильтров каждого слоя\n",
    "        карта стилей формируемого изображения\n",
    "        \n",
    "    gram_target: tf.Tensor, shape = [height, width, channels] - высота, ширина, количество фильтров каждого слоя\n",
    "        матрица Грама соответствующего слоя l стилевого изображения.\n",
    "        \n",
    "    Returns: \n",
    "    ----------\n",
    "    loss: float \n",
    "        Значение функции потерь для стиля на слое\n",
    "    '''\n",
    "    # Масштабирует потери на данном слое по размеру карты объектов и количеству фильтров\n",
    "    height, width, channels = base_style.get_shape().as_list()  # получаем списком размерность карты признаков\n",
    "    gram_style = gram_matrix(base_style) # вычисляем матрицу Грамма для карты признаков стиля\n",
    "\n",
    "    return tf.reduce_mean(tf.square(gram_style - gram_target)) # / (4. * (channels ** 2) * (width * height) ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pXIUX6czZABh"
   },
   "source": [
    "## Применяем перенос стиля к изображениям\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y9r8Lyjb_m0u"
   },
   "source": [
    "### Запуск градиентного спуска\n",
    "Если вы не знакомы с градиентным спуском / обратным распространением или нуждаетесь в переподготовке, вам обязательно стоит заглянуть на этот[замечательный ресурс](https://developers.google.com/machine-learning/crash-course/reducing-loss/gradient-descent).\n",
    "\n",
    "В данном случае мы используем оптимизатор [Adam](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adam) для минимизации потерь. Мы итеративно обновляем наше выходное изображение, чтобы минимизировать наши потери: мы не обновляем веса, связанные сетью, а вместо этого обучаем наше входное изображение, чтобы минимизировать потери. Для этого мы должны знать, как мы вычисляем наши потери и градиенты."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-kGzV6LTp4CU"
   },
   "source": [
    "Мы определим небольшую вспомогательную функцию, которая будет загружать наш контент и изображение стиля, передавать их через нашу сеть, которая затем будет выводить представления элементов контента и стиля из нашей модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O-lj5LxgtmnI"
   },
   "outputs": [],
   "source": [
    "def get_feature_representations(model, content_path, style_path):\n",
    "    '''Вспомогательная функция для вычисления контента и представлений стилевых функций.\n",
    "    \n",
    "    Эта функция загружает и предварительно обрабатывает как контент, так и изображения стилей, расположенных по указанному пути. \n",
    "    Затем он будет передавать в НС, чтобы получить выходы промежуточных слоев.\n",
    "     \n",
    "    Inputs:\n",
    "    ----------\n",
    "    model: \n",
    "        Модель, которую мы используем\n",
    "    content_path: str\n",
    "        Путь к изображению с контетом\n",
    "    style_path: str\n",
    "        Путь к изображению со стилем\n",
    "\n",
    "    Returns:\n",
    "    ----------\n",
    "    style_features: карты признаков???\n",
    "        \n",
    "    content_features: карты признаков???\n",
    "    '''\n",
    "    # Загружаем и обрабатываем изображения контента и стиля при помощи функции load_and_process_img, написанной заранее\n",
    "    content_image = load_and_process_img(content_path)\n",
    "    style_image = load_and_process_img(style_path)\n",
    "\n",
    "    # пакетное вычисление содержимого и функций стиля\n",
    "    style_outputs = model(style_image)\n",
    "    content_outputs = model(content_image)\n",
    "\n",
    "\n",
    "    # Получаем параметры (карты признаков???) стиля и контента из модели  \n",
    "    style_features = [style_layer[0] for style_layer in style_outputs[:num_style_layers]] # список первых эелементов (это что, первая карта???) каждого слоя модели для изображения стиля от 0 слоя до слоя №(количество слоев стиля - 1)\n",
    "    content_features = [content_layer[0] for content_layer in content_outputs[num_style_layers:]] # список первых эелементов (это что, первая карта???) каждого слоя модели для изображения контента от слоя №(количество слоев стиля) до последнего слоя\n",
    "    return style_features, content_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3DopXw7-lFHa"
   },
   "source": [
    "### Вычисление потерь и градиентов\n",
    "Здесь мы используем [**tf.GradientTape**](https://www.tensorflow.org/programmers_guide/eager#computing_gradients) для вычисления градиента. Это позволяет нам воспользоваться преимуществами автоматической дифференциации, доступной при операциях трассировки, для последующего вычисления градиента. Он записывает операции во время прямого прохода, а затем может вычислить градиент нашей функции потерь по отношению к нашему входному изображению для обратного прохода."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oVDhSo8iJunf"
   },
   "outputs": [],
   "source": [
    "def compute_loss(model, loss_weights, init_image, gram_style_features, content_features):\n",
    "    ''' Вычисление общих потерь\n",
    "    \n",
    "    Inputs:\n",
    "    ----------\n",
    "    model:\n",
    "        Используемая модель\n",
    "    loss_weights:\n",
    "        Веса каждого вклада каждой функции потерь.\n",
    "       (вес стиля, вес контента и общий вес вариации - ???)\n",
    "    init_image:\n",
    "    gram_style_features:\n",
    "    content_features:\n",
    "    \n",
    "    Returns:\n",
    "    ----------\n",
    "    loss: float \n",
    "    style_score: float\n",
    "    content_score :float\n",
    "    '''\n",
    "    \n",
    "    style_weight, content_weight = loss_weights # вес стиля, вес контента\n",
    "\n",
    "    # Скармливаем изображение модели \n",
    "    # Это даст нам представление содержимого и стиля на желаемых уровнях. \n",
    "    model_outputs = model(init_image) \n",
    "\n",
    "    style_output_features = model_outputs[:num_style_layers]\n",
    "    content_output_features = model_outputs[num_style_layers:]\n",
    "\n",
    "    style_score = 0\n",
    "    content_score = 0\n",
    "\n",
    "    # Накапливайте потери стиля со всех слоев\n",
    "    # Здесь мы одинаково взвешиваем каждый вклад каждого слоя потерь\n",
    "    weight_per_style_layer = 1.0 / float(num_style_layers)\n",
    "    for target_style, comb_style in zip(gram_style_features, style_output_features):\n",
    "        style_score += weight_per_style_layer * get_style_loss(comb_style[0], target_style)\n",
    "\n",
    "    # Accumulate content losses from all layers \n",
    "    weight_per_content_layer = 1.0 / float(num_content_layers)\n",
    "    for target_content, comb_content in zip(content_features, content_output_features):\n",
    "        content_score += weight_per_content_layer* get_content_loss(comb_content[0], target_content)\n",
    "\n",
    "    style_score *= style_weight\n",
    "    content_score *= content_weight\n",
    "\n",
    "    # Get total loss\n",
    "    loss = style_score + content_score \n",
    "    return loss, style_score, content_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r5XTvbP6nJQa"
   },
   "source": [
    "Теперь легко вычислить градиент:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fwzYeOqOUH9_"
   },
   "outputs": [],
   "source": [
    "def compute_grads(cfg):\n",
    "    ''' Вычисление градиента\n",
    "    \n",
    "    Inputs:\n",
    "    ----------\n",
    "    cfg:      \n",
    "      \n",
    "    Returns:\n",
    "    ----------\n",
    "    tape.gradient(total_loss\n",
    "    cfg['init_image'])\n",
    "    all_loss\n",
    "  \n",
    "  \n",
    "  '''\n",
    "    with tf.GradientTape() as tape: \n",
    "        all_loss = compute_loss(**cfg)\n",
    "    # Compute gradients wrt input image\n",
    "    total_loss = all_loss[0]\n",
    "    return tape.gradient(total_loss, cfg['init_image']), all_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T9yKu2PLlBIE"
   },
   "source": [
    "### Цикл оптимизации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pj_enNo6tACQ"
   },
   "outputs": [],
   "source": [
    "import IPython.display\n",
    "\n",
    "def run_style_transfer(content_path, \n",
    "                       style_path,\n",
    "                       num_iterations=1000,\n",
    "                       content_weight=1e3, \n",
    "                       style_weight=1e-2): \n",
    "    \n",
    "    '''Процесс переноса стиля\n",
    "    \n",
    "    Inputs:\n",
    "    ----------\n",
    "    content_path:  \n",
    "    style_path: \n",
    "    num_iterations=1000: \n",
    "    content_weight=1e3: \n",
    "    style_weight=1e-2: \n",
    "      \n",
    "    Returns:\n",
    "    ----------\n",
    "    tape.gradient(total_loss\n",
    "    cfg['init_image'])\n",
    "    all_loss\n",
    "  \n",
    "  \n",
    "  '''\n",
    "  # We don't need to (or want to) train any layers of our model, so we set their\n",
    "  # trainable to false. \n",
    "  model = get_model() \n",
    "  for layer in model.layers:\n",
    "    layer.trainable = False\n",
    "  \n",
    "  # Get the style and content feature representations (from our specified intermediate layers) \n",
    "  style_features, content_features = get_feature_representations(model, content_path, style_path)\n",
    "  gram_style_features = [gram_matrix(style_feature) for style_feature in style_features]\n",
    "  \n",
    "  # Set initial image\n",
    "  init_image = load_and_process_img(content_path)\n",
    "  init_image = tf.Variable(init_image, dtype=tf.float32)\n",
    "  # Create our optimizer\n",
    "  opt = tf.train.AdamOptimizer(learning_rate=5, beta1=0.99, epsilon=1e-1)\n",
    "\n",
    "  # For displaying intermediate images \n",
    "  iter_count = 1\n",
    "  \n",
    "  # Store our best result\n",
    "  best_loss, best_img = float('inf'), None\n",
    "  \n",
    "  # Create a nice config \n",
    "  loss_weights = (style_weight, content_weight)\n",
    "  cfg = {\n",
    "      'model': model,\n",
    "      'loss_weights': loss_weights,\n",
    "      'init_image': init_image,\n",
    "      'gram_style_features': gram_style_features,\n",
    "      'content_features': content_features\n",
    "  }\n",
    "    \n",
    "  # For displaying\n",
    "  num_rows = 2\n",
    "  num_cols = 5\n",
    "  display_interval = num_iterations/(num_rows*num_cols)\n",
    "  start_time = time.time()\n",
    "  global_start = time.time()\n",
    "  \n",
    "  norm_means = np.array([103.939, 116.779, 123.68])\n",
    "  min_vals = -norm_means\n",
    "  max_vals = 255 - norm_means   \n",
    "  \n",
    "  imgs = []\n",
    "  for i in range(num_iterations):\n",
    "    grads, all_loss = compute_grads(cfg)\n",
    "    loss, style_score, content_score = all_loss\n",
    "    opt.apply_gradients([(grads, init_image)])\n",
    "    clipped = tf.clip_by_value(init_image, min_vals, max_vals)\n",
    "    init_image.assign(clipped)\n",
    "    end_time = time.time() \n",
    "    \n",
    "    if loss < best_loss:\n",
    "      # Update best loss and best image from total loss. \n",
    "      best_loss = loss\n",
    "      best_img = deprocess_img(init_image.numpy())\n",
    "\n",
    "    if i % display_interval== 0:\n",
    "      start_time = time.time()\n",
    "      \n",
    "      # Use the .numpy() method to get the concrete numpy array\n",
    "      plot_img = init_image.numpy()\n",
    "      plot_img = deprocess_img(plot_img)\n",
    "      imgs.append(plot_img)\n",
    "      IPython.display.clear_output(wait=True)\n",
    "      IPython.display.display_png(Image.fromarray(plot_img))\n",
    "      print('Iteration: {}'.format(i))        \n",
    "      print('Total loss: {:.4e}, ' \n",
    "            'style loss: {:.4e}, '\n",
    "            'content loss: {:.4e}, '\n",
    "            'time: {:.4f}s'.format(loss, style_score, content_score, time.time() - start_time))\n",
    "  print('Total time: {:.4f}s'.format(time.time() - global_start))\n",
    "  IPython.display.clear_output(wait=True)\n",
    "  plt.figure(figsize=(14,4))\n",
    "  for i,img in enumerate(imgs):\n",
    "      plt.subplot(num_rows,num_cols,i+1)\n",
    "      plt.imshow(img)\n",
    "      plt.xticks([])\n",
    "      plt.yticks([])\n",
    "      \n",
    "  return best_img, best_loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vSVMx4burydi"
   },
   "outputs": [],
   "source": [
    "best, best_loss = run_style_transfer(content_path, \n",
    "                                     style_path, num_iterations=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dzJTObpsO3TZ"
   },
   "outputs": [],
   "source": [
    "Image.fromarray(best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dCXQ9vSnQbDy"
   },
   "source": [
    "To download the image from Colab uncomment the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SSH6OpyyQn7w"
   },
   "outputs": [],
   "source": [
    "#from google.colab import files\n",
    "#files.download('wave_turtle.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LwiZfCW0AZwt"
   },
   "source": [
    "## Visualize outputs\n",
    "We \"deprocess\" the output image in order to remove the processing that was applied to it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lqTQN1PjulV9"
   },
   "outputs": [],
   "source": [
    "def show_results(best_img, content_path, style_path, show_large_final=True):\n",
    "  plt.figure(figsize=(10, 5))\n",
    "  content = load_img(content_path) \n",
    "  style = load_img(style_path)\n",
    "\n",
    "  plt.subplot(1, 2, 1)\n",
    "  imshow(content, 'Content Image')\n",
    "\n",
    "  plt.subplot(1, 2, 2)\n",
    "  imshow(style, 'Style Image')\n",
    "\n",
    "  if show_large_final: \n",
    "    plt.figure(figsize=(10, 10))\n",
    "\n",
    "    plt.imshow(best_img)\n",
    "    plt.title('Output Image')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i6d6O50Yvs6a"
   },
   "outputs": [],
   "source": [
    "show_results(best, content_path, style_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tyGMmWh2Pss8"
   },
   "source": [
    "## Try it on other images\n",
    "Image of Tuebingen \n",
    "\n",
    "Photo By: Andreas Praefcke [GFDL (http://www.gnu.org/copyleft/fdl.html) or CC BY 3.0  (https://creativecommons.org/licenses/by/3.0)], from Wikimedia Commons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x2TePU39k9lb"
   },
   "source": [
    "### Starry night + Tuebingen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ES9dC6ZyJBD2"
   },
   "outputs": [],
   "source": [
    "best_starry_night, best_loss = run_style_transfer('/tmp/nst/Tuebingen_Neckarfront.jpg',\n",
    "                                                  '/tmp/nst/1024px-Van_Gogh_-_Starry_Night_-_Google_Art_Project.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X8w8WLkKvzXu"
   },
   "outputs": [],
   "source": [
    "show_results(best_starry_night, '/tmp/nst/Tuebingen_Neckarfront.jpg',\n",
    "             '/tmp/nst/1024px-Van_Gogh_-_Starry_Night_-_Google_Art_Project.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QcXwvViek4Br"
   },
   "source": [
    "### Pillars of Creation + Tuebingen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vJ3u2U-gGmgP"
   },
   "outputs": [],
   "source": [
    "best_poc_tubingen, best_loss = run_style_transfer('/tmp/nst/Tuebingen_Neckarfront.jpg', \n",
    "                                                  '/tmp/nst/Pillars_of_creation_2014_HST_WFC3-UVIS_full-res_denoised.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pQUq3KxpGv2O"
   },
   "outputs": [],
   "source": [
    "show_results(best_poc_tubingen, \n",
    "             '/tmp/nst/Tuebingen_Neckarfront.jpg',\n",
    "             '/tmp/nst/Pillars_of_creation_2014_HST_WFC3-UVIS_full-res_denoised.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bTZdTOdW3s8H"
   },
   "source": [
    "### Kandinsky Composition 7 + Tuebingen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bt9mbQfl7exl"
   },
   "outputs": [],
   "source": [
    "best_kandinsky_tubingen, best_loss = run_style_transfer('/tmp/nst/Tuebingen_Neckarfront.jpg', \n",
    "                                                  '/tmp/nst/Vassily_Kandinsky,_1913_-_Composition_7.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qnz8HeXSXg6P"
   },
   "outputs": [],
   "source": [
    "show_results(best_kandinsky_tubingen, \n",
    "             '/tmp/nst/Tuebingen_Neckarfront.jpg',\n",
    "             '/tmp/nst/Vassily_Kandinsky,_1913_-_Composition_7.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cg68lW2A3s8N"
   },
   "source": [
    "### Pillars of Creation + Sea Turtle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dl0DUot_bFST"
   },
   "outputs": [],
   "source": [
    "best_poc_turtle, best_loss = run_style_transfer('/tmp/nst/Green_Sea_Turtle_grazing_seagrass.jpg', \n",
    "                                                  '/tmp/nst/Pillars_of_creation_2014_HST_WFC3-UVIS_full-res_denoised.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UzJfE0I1bQn8"
   },
   "outputs": [],
   "source": [
    "show_results(best_poc_turtle, \n",
    "             '/tmp/nst/Green_Sea_Turtle_grazing_seagrass.jpg',\n",
    "             '/tmp/nst/Pillars_of_creation_2014_HST_WFC3-UVIS_full-res_denoised.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sElaeNX-4Vnc"
   },
   "source": [
    "## Key Takeaways\n",
    "\n",
    "### What we covered:\n",
    "\n",
    "* We built several different loss functions and used backpropagation to transform our input image in order to minimize these losses\n",
    "  * In order to do this we had to load in a **pretrained model** and use its learned feature maps to describe the content and style representation of our images.\n",
    "    * Our main loss functions were primarily computing the distance in terms of these different representations\n",
    "* We implemented this with a custom model and **eager execution**\n",
    "  * We built our custom model with the Functional API \n",
    "  * Eager execution allows us to dynamically work with tensors, using a natural python control flow\n",
    "  * We manipulated tensors directly, which makes debugging and working with tensors easier. \n",
    "* We iteratively updated our image by applying our optimizers update rules using **tf.gradient**. The optimizer minimized a given loss with respect to our input image. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U-y02GWonqnD"
   },
   "source": [
    "\n",
    "**[Image of Tuebingen](https://commons.wikimedia.org/wiki/File:Tuebingen_Neckarfront.jpg)** \n",
    "Photo By: Andreas Praefcke [GFDL (http://www.gnu.org/copyleft/fdl.html) or CC BY 3.0  (https://creativecommons.org/licenses/by/3.0)], from Wikimedia Commons\n",
    "\n",
    "**[Image of Green Sea Turtle](https://commons.wikimedia.org/wiki/File:Green_Sea_Turtle_grazing_seagrass.jpg)**\n",
    "By P.Lindgren [CC BY-SA 3.0 (https://creativecommons.org/licenses/by-sa/3.0)], from Wikimedia Commons\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IpUD9W6ZkeyM"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Neural Style Transfer with Eager Execution",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
